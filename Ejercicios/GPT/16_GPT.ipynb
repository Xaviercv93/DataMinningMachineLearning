{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 16 GPT: Generative Pre-trained Transformer\n",
    "\n",
    "GPT is a model and approach developed by OpenAI. It is primarily known for its capabilities in generating coherent and contextually relevant text over long passages.\n",
    "\n",
    "* Architecture: GPT is based on the Transformer architecture. Unlike some other models that use both an encoder and a decoder, GPT exclusively utilizes the decoder part of the Transformer for its tasks.\n",
    "* Pre-training and Fine-tuning:\n",
    "    - Pre-training: GPT is first pre-trained on a large corpus of text (like books, articles, websites, etc.). During this phase, it learns to predict the next word in a sentence. This process enables the model to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.\n",
    "    - Fine-tuning: After pre-training, the model can be fine-tuned on a specific task, such as translation, question-answering, or summarization, using a smaller, task-specific dataset.\n",
    "\n",
    "* Autoregressive Nature: GPT generates text in an autoregressive manner. This means it produces one word at a time and uses what it's generated so far as a context to generate the next word.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "* Generative Abilities: As the name suggests, GPT excels at generating text. It can produce text that is often indistinguishable from what a human might write.\n",
    "* Few-Shot Learning: Introduced with GPT-3, this capability allows the model to perform tasks even when provided with very few examples (sometimes as few as one). By just specifying a task in natural language, GPT-3 can often understand and perform the task without explicit fine-tuning.\n",
    "* Versatility: Unlike many models that are trained for a specific task, GPT models, especially GPT-3, are versatile and can handle a wide range of tasks without task-specific training. This includes writing essays, answering questions, creating poetry, generating code, and much more.\n",
    "\n",
    "#### Versions:\n",
    "\n",
    "* GPT: The original model introduced by OpenAI.\n",
    "\n",
    "* GPT-2: A larger and more powerful version that garnered significant attention due to its impressive text generation capabilities. OpenAI initially withheld the fully-trained model due to concerns about misuse, but later released it given the broader community's responsible usage.\n",
    "\n",
    "* GPT-3: The third iteration with 175 billion parameters, making it one of the largest models ever created. It introduced the concept of few-shot and zero-shot learning, further advancing the state-of-the-art in various NLP tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise: Exploring Creative Writing with GPT\n",
    "\n",
    "Your task is to use OpenAI's GPT model to generate creative content. You'll explore various prompts and settings to see how GPT responds and creates different outputs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load GPT-2 Model and Tokenizer. GPT-2 is freely available in Hugging Face's model hub and is still highly effective."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d82d2a2b81f4edfbdf135be305c4b2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\handson310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ivanc\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ba673cc83fd4977b750da4f571091c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9950b779ec9741e98643d96d7525acc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e08f34839594005ae4c82efb7439202"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "849f96de6b9e4190b3948d7676171a43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = 'gpt2-medium'  # You can start with 'gpt2' (smaller) and then experiment with larger models\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate Creative Content Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def generate_creative_content(prompt, max_length=150, temperature=1.0):\n",
    "    \"\"\"Generate creative content using GPT-2 based on a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode and print the generated text\n",
    "    generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment:\n",
    "Use various prompts and observe GPT's creative capabilities.\n",
    "Change parameters like *max_length* and *temperature* to see their impact. (Note: A higher temperature value makes output more random, while a lower value makes it more deterministic.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time, in a kingdom far away,\n",
      " there lived a king who was a great warrior. He was a great warrior, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king. He was a great king, and he was a great king\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: In a dystopian future, where AI rules the world,\n",
      " the only way to survive is to become a cyborg.\n",
      "\n",
      "The game is set in a dystopian future where AI rules the world, and the only way to survive is to become a cyborg.\n",
      "\n",
      "The game is set in a dystopian future where AI rules the world, and the only way to survive is to become a cyborg.\n",
      "\n",
      "The game is set in a dystopian future where AI rules the world, and the only way to survive is to become a cyborg.\n",
      "\n",
      "The game is set in a dystopian future where AI rules the world, and the only way to survive is to become a cyborg.\n",
      "\n",
      "The game is set in a dystopian future where\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: The last dinosaur on Earth was not like the others. It\n",
      " was a giant, bipedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadrupedal, quadruped\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Prompt: Deep beneath the ocean waves, a secret civilization\n",
      " has been building a vast network of underground tunnels. The tunnels are filled with powerful weapons and technology, and are guarded by powerful creatures.\n",
      "\n",
      "The game is set in the same universe as the original Fallout 3, but with a few changes. The game is set in the year 2277, and the player is a member of the Brotherhood of Steel. The player is tasked with finding a way to infiltrate the Brotherhood's underground network, and to stop the Brotherhood from destroying the world.\n",
      "\n",
      "The game is set in the same universe as the original Fallout 3, but with a few changes. The game is set in the year 2277, and the player is a member of the Brotherhood of Steel.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time, in a kingdom far away,\",\n",
    "    \"In a dystopian future, where AI rules the world,\",\n",
    "    \"The last dinosaur on Earth was not like the others. It\",\n",
    "    \"Deep beneath the ocean waves, a secret civilization\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(generate_creative_content(prompt))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Discussion and Analysis:\n",
    "- Analyze the quality of the generated text: coherence, relevancy, and creativity.\n",
    "- Discuss how different prompts influence the direction of the story.\n",
    "- Experiment with custom prompts to generate different genres of creative content (e.g., horror, sci-fi, romance)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tasks:\n",
    "* Use a larger GPT-2 variant (gpt2-large or gpt2-xl) and compare the quality of outputs.\n",
    "* Incorporate user feedback loops, where after getting an initial piece of text, they can provide a follow-up prompt to continue or steer the story."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
